# 摘要

基于图像的虚拟试穿对于在线购物越来越重要。它旨在合成穿着特定服装的特定人物的图像。最近，扩散模型为基础的方法变得流行起来，因为它们在图像合成任务上表现出色。然而，这些方法通常使用额外的图像编码器，并依赖于交叉注意力机制进行从服装到人物图像的纹理转移，这影响了试穿的效率和保真度。为了解决这些问题，我们提出了一种Texture-Preserving Diffusion（TPD）模型用于虚拟试穿，它增强了结果的保真度，并且不引入额外的图像编码器。因此，我们从两个方面做出了贡献。首先，我们建议沿空间维度连接遮罩人物和参考服装图像，并利用生成的图像作为扩散模型去噪UNet的输入。这使得扩散模型中包含的原始自注意力层能够实现高效准确的纹理转移。其次，我们提出了一种基于扩散的新方法，根据人物和参考服装图像预测精确的修补遮罩，进一步增强了试穿结果的可靠性。此外，我们将遮罩预测和图像合成集成到一个单一的紧凑模型中。实验结果表明，我们的方法可以应用于各种试穿任务，如服装到人物和人物到人物的试穿，并且在流行的VITON、VITON-HD数据库上明显优于最先进的方法。代码可在\href{https://github.com/Gal4way/TPD}{\texttt{https://github.com/Gal4way/TPD}}找到。

# 引言
## 背景介绍
随着在线购物的普及，基于图像的虚拟试穿近年来在研究界引起了广泛关注。图像虚拟试穿的目标是以逼真的方式将人物图像中的衣服替换为指定的服装。它有潜力显著提升顾客的在线购物体验；然而，这一任务仍然具有挑战性。一个关键问题是，参考服装必须自然变形以适应指定人物的身体形状和姿势。此外，参考服装上的图案和纹理细节在虚拟试穿过程中应保持并逼真地扭曲。

为了克服这一挑战，现有方法通常在图像合成之前执行服装变形，如图~\ref{fig_comparision_attention}(a)所示。然而，服装变形会产生难以在合成阶段纠正的伪影。因此，最近的工作开始探索基于强大的扩散模型的无变形方法。它们通常利用去噪UNet中的交叉注意力机制将参考服装的纹理转移到人物图像的对应区域。为了提取参考服装的纹理特征，DCI-VTON和MGD直接利用原始的CLIP编码器，而LaDI-VTON和TryOnDiffusion则采用额外的图像编码器，例如Vision Transformer（VIT）或额外的UNet模型。

然而，高保真度试穿图像的有效生成仍然是未被充分探索的课题。首先，使用CLIP图像编码器提取特征会丢失细粒度纹理，因为该编码器最初是为了与粗略字幕的整体特征对齐而训练的。此外，使用专门的图像编码器会增加计算成本。其次，现有方法通常通过粗略估计的修补遮罩从人物图像中去除原始服装。虽然它可能无法覆盖原始人物图像的每个纹理，但通常会去除与服装无关的纹理，例如纹身和肌肉结构，这进一步影响了试穿结果的保真度。

因此，我们提出了一种Texture-Preserving Diffusion（TPD）模型，用于高保真度虚拟试穿，以解决这些挑战。首先，我们提出了一种Self-Attention-based Texture Transfer（SATT）方法。与现有方法相反，我们在我们的方法中放弃了服装变形和专门的服装图像编码器。相反，我们发现扩散模型内部的原始自注意力块对于服装纹理转移更为有效和高效。具体来说，我们沿空间维度连接遮罩人物和参考服装图像，将生成的图像馈送到扩散模型中。然后，我们利用Stable Diffusion（SD）模型中去噪UNet中的强大自注意力块来捕获合成图像中像素之间的长距离相关性。这种策略将参考服装视为同一图像中遮罩人物的上下文，并使得扩散模型的前向传递中能够高效地从服装向人物图像进行纹理转移。此外，由于UNet包含具有多个分辨率的自注意力块，它有助于在不同特征尺度之间实现更有效的纹理转移。在实验部分，我们展示了SATT在生成具有复杂纹理、图案和具有挑战性的身体姿势变化的高保真度试穿图像方面的能力。

其次，我们提出了一种Decoupled Mask Prediction（DMP）方法，用于自动确定每个人物-服装图像对的准确修补区域。由于准确的遮罩取决于原始人物和参考服装图像，我们以分离的方式预测这个遮罩。具体来说，DMP从初始随机噪声逐步去噪到由参考服装确定的修补区域。我们还利用人体解析工具获得人物图像中原始服装的区域。最后，我们将两个区域的并集作为最终的修补遮罩。与现有方法只采用由原始人物图像确定的遮罩不同，DMP预测的遮罩适应了遇到的服装，使我们能够尽可能保留身份信息。在实验部分，我们展示了DMP相对于现有方法能够保留手指、手臂和纹身，增强了合成图像的保真度。

我们的主要贡献总结如下。首先，我们提出了一种新颖的基于扩散且无变形的方法，实现了更有效和准确的虚拟试穿。其次，我们探讨了粗略修补遮罩对合成图像保真度的影响，并提出了一种准确遮罩预测的新方法。第三，我们的方法在流行的VITON和VITON-HD数据库上始终优于最先进方法，合成图像的逼真性和连贯性更高。

# 相关工作
## 图像虚拟试穿
### 基于变形的方法
现有的基于图像的虚拟试穿方法可以分为基于变形和无变形两种方法。

基于变形的方法在图像合成之前执行服装变形。它们通常采用两阶段框架：第一阶段将服装图像变形到人物图像的身体上，而第二阶段通过融合变形后的服装和人物图像来合成最终图像。Thin Plate Spline（TPS）、流场图和关键点等技术有助于服装变形。在图像合成阶段，一些方法通过提供额外的线索（如人体解析图）来提高合成图像的保真度，而另一些通过修改生成模型的结构（如引入额外的归一化层）来提高图像质量。最近，研究人员开始在图像合成阶段使用扩散模型替代生成对抗网络，因为扩散模型具有强大的图像生成能力。因此，它们获得了更高质量和逼真度的试穿图像。基于变形的方法的主要缺点是服装变形产生的伪影难以在图像合成阶段纠正。

相反，无变形的方法通常基于扩散模型，通过遮罩原始服装并使用额外的图像编码器和扩散模型的交叉注意力块将服装纹理转移到遮罩区域。Baldrati等人采用了原始的CLIP文本编码器来实现多模态虚拟试穿。类似地，Gou等人将CLIP文本编码器替换为CLIP图像编码器，以提取图像特征。Morelli等人则引入了额外的VIT模型来补充CLIP编码器。然而，CLIP图像编码器的特征提取是基于粗略的文本描述训练的，导致提取的特征也较为粗糙，进而影响了试穿图像的纹理。Zhu等人训练了一个新的扩散模型，并引入了一个额外的UNet模型来替换CLIP图像编码器，以便从服装图像中进行多尺度特征提取。但是，这种扩大的模型架构也带来了额外的计算成本。

本文针对现有无变形虚拟试穿方法中的保真度问题进行了研究。我们提出利用扩散模型内部的原始自注意力块实现更强大、更高效的服装纹理转移。我们还介绍了一种根据具体的人物-服装配对自动确定准确修补区域的方法，从而使模型能够生成高保真度的图像。

### 扩散模型
扩散模型吸引了大量研究关注，因为它们能够生成高质量的图像并实现稳定的训练收敛。首先提出了去噪扩散概率模型（DDPM）来对图像生成过程建模。随后，提出了去噪扩散隐式模型（DDIM）和伪数值方法用于扩散模型（PNDM）来加速生成过程。最近，引入了潜在扩散模型来在预训练的变分自动编码器（VAE）的潜在空间中执行扩散过程，从而实现更高的计算效率和生成图像质量。

潜在扩散模型已经应用于各种图像生成任务，并且许多研究旨在提高生成过程的可控性。例如，Yang等人将SD模型中的CLIP文本编码器替换为CLIP图像编码器，使模型能够根据图像条件生成图像。Karras等人采用了预训练的VAE编码器来补充CLIP图像编码器，提高了高保真度图像的生成效果。最近，Zhang等人提出了ControlNet模型，引入了一个额外的网络，将图像条件作为显式引导注入到冻结的SD模型中。ControlNet在输入和输出结构对齐的任务中表现良好，但在虚拟试穿任务中可能会遇到困难，因为人物和服装图像之间的姿势差异很大。

本研究解决了基于SD模型的虚拟试穿挑战。与上述研究相比，我们在不使用专门的图像编码器的情况下生成高保真度的试穿图像。此外，我们的方法鲁棒性强，能够处理重要的姿势差异。


![我们框架的概览。 (a) 在训练阶段，我们从原始人物图像 $S$ 和一个随机增强的面具 $c_m$ 开始。 $c_m$ 是通过在原始服装区域 $M_s$ 和边界框 $b_s$ 之间进行插值得到的。增强的面具 $c_m$，遮罩人物图像 $c_m \odot S$，姿势图 $c_p$ 和密集姿势 $c_d$ 作为去噪 UNet 的辅助输入。此外，参考服装 $C$ 与每个辅助输入沿着空间维度串联，作为自注意机制的上下文。(b) 推理阶段分为两个阶段。在第一阶段，我们为新服装 $C^*$ 预测人物上的服装区域 $m_0^{s1}$。通过在 $m_0^{s1}$ 和 $M_s$ 之间进行逐元素乘法，得到 $c_m^{s2}$。在第二阶段，$c_m^{s2}$ 被用作准确的修复面具，使扩散模型能够产生高保真度的试穿图像。为了清晰起见，我们在两个阶段的结果中省略了预测的串联服装。](Figures/3_model_dev.pdf)
**图1** 我们框架

## 方法
### 初步：扩散模型
DDPMs 从正态分布的随机噪声中迭代地恢复图像。为了提高训练和推理速度，最近的扩散模型，例如 SD 模型，在预训练的自动编码器 的编码潜空间中操作。
SD 包括两个核心组件：VAE 和去噪 UNet。具体地，VAE 编码器 $E$ 首先将输入图像 $x \in \textbf{R}^{3 \times H \times W}$ 编码为潜在表示 $z = E(x) \in \textbf{R}^{4 \times h \times w}$。经过 $T$ 个扩散步骤后，$z$ 通常发展为各向同性的高斯噪声 $z_T$。然后，基于文本的去噪 UNet $\epsilon_\theta$ 被应用于逐步预测在每个时间步 $t=1, ..., T$ 中添加的噪声，并最终恢复 $z'$。VAE 解码器 $D$ 使用 $z'$ 作为输入重构原始图像，即 $x' = D(z')$。
对于修复任务，除了 $z$ 外，U-Net 还使用了两个额外的输入，即修复面具 $m$ 和修复背景 $E((m \odot x)$。
目标函数定义如下：
$$ 
L_{SD} = \mathbb{E}_{z,\epsilon\sim\mathcal{N}(0,1),t}[\|\epsilon - \epsilon_\theta(z_t, E(m \odot x), m, t, e)\|_2^2],
\label{eq:1}
$$
其中 $\epsilon$ 表示在此步骤中添加的真实噪声，$\odot$ 表示逐元素乘法，$e$ 表示使用 CLIP 编码器获得的嵌入。

### 概述
我们的 TPD 模型的概览如图所示。在本例中，我们采用 SD 模型 作为骨干。我们将原始人物图像表示为 $S \in \textbf{R}^{3 \times H \times W}$，参考服装图像表示为 $C^* \in \textbf{R}^{3 \times H \times W}$，并将穿着参考服装的合成人物图像表示为 $I^* \in \textbf{R}^{3 \times H \times W}$。在实践中，以 $<S, C^*, I^*>$ 的形式收集三元组数据是具有挑战性的。为了解决这个问题，现有的数据库通常采用 $<S,C>$ 的成对数据形式，其中 $C$ 指的是包含 $S$ 中人物穿着相同服装的服装图像，如图所示。

在接下来的几节中，我们分别介绍了基于自注意力的纹理转移 (SATT) 方法和解耦合面具预测 (DMP) 方法。

### 基于自注意力的纹理转移
SD 模型的去噪 UNet 在每个分辨率级别包含卷积、自注意力和交叉注意力块。现有方法通常利用交叉注意力块实现从服装到人物的纹理转移。因此，它们侧重于提升专门的服装图像编码器 的特征提取能力，其输出用作交叉注意力操作的键和值。然而，提升专门的服装图像编码器的能力通常会增加额外的计算成本。我们认为现有工作忽视了自注意力块的潜在好处。

本节提出利用去噪 UNet 中固有的自注意力块进行更准确、更高效的虚拟试穿。从根本上讲，我们将参考服装和人物图像中未遮挡的区域都视为修复任务的上下文。具体而言，我们首先沿空间维度串联服装图像 $C$ 和遮罩人物图像 $c_m \odot S$。然后，我们将得到的图像馈送到 UNet 中。这使得 $C$ 成为组合图像中的上下文之一。因此，扩散模型的任务变为从随机高斯噪声中重构人物和服装图像，如图~\ref{fig_model} 所示。结果，UNet 的卷积块提取服装的纹理，而自注意力块有效地将纹理从服装转移到人物图像。如图~\ref{fig_comparision_attention} 所示，自注意力操作可以表示为：

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V,$$

其中 $Q,K,V \in \mathbb{R}^{p \times d}$ 是从相同潜在特征图重塑的堆叠向量，$p$ 是特征图中的像素数，$d$ 表示向量维度。通过这种方式，考虑了特征图中每一对像素的相关性，自然实现了在同一图像中从服装区域到人物区域的纹理转移。

或者，$C$ 和 $c_m \odot S$ 可以沿通道维度进行串联。然而，如 所述，$C$ 和 $c_m\odot S$ 中的像素不是空间对齐的；因此，使用卷积或自注意力操作很难将 $C$ 中的纹理转移到 $c_m\odot S$ 中的遮罩区域。第~\ref{sec:experiments} 节演示了我们的策略比沿通道维度串联操作显著更好的效果。

### 解耦掩码预测
现有方法 通常使用面具来移除人物图像中的原始服装。因此，准确性是虚拟试穿任务性能的关键。然而，现有方法倾向于粗略地估计每个人物图像的一个面具，并将其应用于所有参考服装。如图~\ref{fig_comparison_mask} 所示，这种粗略的面具可能覆盖一些背景和身体部位区域，导致信息的不必要丢失。这些问题会影响合成试穿图像 $I^*$ 的保真度。

我们提出了一种方法来为每个特定的 $<S,C^*>$ 对预测准确的面具，以解决这个问题。假设人物同时穿着原始服装和新服装，则准确的修复面具等于两个服装区域的并集。由于原始服装区域 $M_s$ 可以通过 $S$ 进行人体解析获得，我们的方法旨在预测新服装的服装区域。

除了为图像合成任务预测潜在的 $z$ 外，我们的方法还引入了一个额外的通道 $m$，专门用于预测目标人物上的新服装的服装区域，如图~\ref{fig_model} 所示。值得注意的是，训练数据的形式是 $<S,C>$，训练阶段预测的面具正是 $S$ 中原始服装的服装区域。相比之下，推理阶段的数据形式是 $<S,C^*>$。因此，我们在测试期间采用以下两阶段预测流程。
如图~\ref{fig_model} 所示，第一阶段中，我们使用边界框作为初始修复面具 $c_m^{s1}$。我们的模型从随机高斯噪声中迭代地预测粗糙的试穿图像和新服装 $C^*$ 的服装区域 $m_0^{s1}$。在第二阶段中，我们利用 $m_0^{s1}$ 和 $M_s$ 的并集，得到当前人物-服装图像对的准

综上所述，通过 DMP，我们可以获得准确的修复面具，在对原始人物图像进行最小修改的情况下实现无翘曲的虚拟试穿。

| 数据库 | 方法 | SSIM$\uparrow$ | FID$\downarrow$ | LPIPS$\downarrow$ |
|--------|------|---------------|----------------|-------------------|
| VITON  | CP-VTON | 0.78 | 24.43 | - |
| VITON  | ClothFlow | 0.84 | 14.43 | - |
| VITON  | ACGPN | 0.84 | 15.67 | 0.11 |
| VITON  | SDAFN | 0.85 | 10.55 | 0.09 |
| VITON  | PF-AFN | 0.87 | 10.09 | 0.08 |
| VITON  | Paint-by-Example | 0.83 | 12.56 | 0.12 |
| VITON  | **我们的方法** | **0.89** | **9.58** | **0.07** |
| VITON-HD  | CP-VTON | 0.79 | 30.25 | 0.14 |
| VITON-HD  | PF-AFN | 0.85 | 11.30 | 0.08 |
| VITON-HD  | VITON-HD | 0.84 | 11.65 | 0.11 |
| VITON-HD  | HR-VITON | 0.87 | 10.91 | 0.10 |
| VITON-HD  | LaDI-VTON | 0.87 | 9.41 | 0.09 |
| VITON-HD  | DCI-VTON | 0.88 | 8.78 | 0.08 |
| VITON-HD  | Paint-by-Example | 0.84 | 12.15 | 0.13 |
| VITON-HD  | **我们的方法** | **0.90** | **8.54** | **0.07** |


